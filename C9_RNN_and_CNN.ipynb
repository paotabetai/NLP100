{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtIZ4zgMQUi5fdq5ZkDkcB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oMSTuiyvDdd",
        "outputId": "4113711b-95cc-4e0a-f04f-1b0fd8b2c2ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "## 同一ディレクトリ内にhttps://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g からダウンロードした学習済み単語ベクトルを配置しているものとする\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHg_7sfDvcx0",
        "outputId": "5cc71695-3667-47c3-9fec-8cb5cfa13dad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-30 12:55:37--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29224203 (28M) [application/x-httpd-php]\n",
            "Saving to: ‘NewsAggregatorDataset.zip’\n",
            "\n",
            "NewsAggregatorDatas 100%[===================>]  27.87M  27.0MB/s    in 1.0s    \n",
            "\n",
            "2023-01-30 12:55:39 (27.0 MB/s) - ‘NewsAggregatorDataset.zip’ saved [29224203/29224203]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip NewsAggregatorDataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTOdSlI6v0w8",
        "outputId": "0d6abf67-43a2-4b20-e9f8-99ff1f2a92b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  NewsAggregatorDataset.zip\n",
            "  inflating: 2pageSessions.csv       \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._2pageSessions.csv  \n",
            "  inflating: newsCorpora.csv         \n",
            "  inflating: __MACOSX/._newsCorpora.csv  \n",
            "  inflating: readme.txt              \n",
            "  inflating: __MACOSX/._readme.txt   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "CATEGORIES = [\n",
        "    'b',\n",
        "    't',\n",
        "    't',\n",
        "    'm'\n",
        "]\n",
        "\n",
        "with open('./newsCorpora.csv') as f:\n",
        "   all_news = [news for news in f.readlines() if news.split('\\t')[3] in TARGET_PUBLISHERS]\n",
        "\n",
        "random.shuffle(all_news)\n",
        "\n",
        "train_data_num = len(all_news)  * 8 // 10\n",
        "valid_data_num = len(all_news)  * 9 // 10\n",
        "train_data = all_news[: train_data_num]\n",
        "valid_data = all_news[train_data_num : valid_data_num]\n",
        "test_data = all_news[valid_data_num :]\n",
        "\n",
        "with open('./train.txt', mode='w') as f:\n",
        "  f.writelines('\\t'.join(train_data))\n",
        "\n",
        "with open('./valid.txt', mode='w') as f:\n",
        "  f.writelines('\\t'.join(valid_data))\n",
        "\n",
        "with open('./test.txt', mode='w') as f:\n",
        "  f.writelines('\\t'.join(test_data))\n",
        "\n",
        "print(f'学習データ事例数: {len(train_data)}')\n",
        "print(f'評価データ事例数: {len(test_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpXCWqOqv3XW",
        "outputId": "b1092968-7a29-47ad-988c-3ce8812c5954"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "学習データ事例数: 10684\n",
            "評価データ事例数: 1336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 80\n",
        "\n",
        "def calculate_id(filepath):\n",
        "  word_dic = {}\n",
        "  result = {}\n",
        "  with open(filepath, mode='r') as f:\n",
        "    data = f.readlines()\n",
        "    for row in data:\n",
        "      for word in row.split('\\t')[2].strip(\"'\").split(' '):\n",
        "        if not word in word_dic:\n",
        "          word_dic[word] = 0\n",
        "        word_dic[word] += 1\n",
        "  sorted_words = sorted([(key, value) for key, value in word_dic.items()], key=lambda x: x[1], reverse=True)\n",
        "  i = 0\n",
        "  for word in sorted_words:\n",
        "    if word[1] > 1:\n",
        "      i += 1\n",
        "      result[word[0]] = i\n",
        "    else:\n",
        "      result[word[0]] = 0\n",
        "  return result, i\n",
        "\n",
        "words, T = calculate_id('./train.txt')\n",
        "\n",
        "def get_id_by_word(word):\n",
        "  try:\n",
        "    return words[word]\n",
        "  except:\n",
        "    return 0\n"
      ],
      "metadata": {
        "id": "_LceNXMFv5pS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 81\n",
        "## TODO: \n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "def vector_emb(id, dw):\n",
        "  if id > dw:\n",
        "    id = 0\n",
        "  x_class = np.array([id])\n",
        "  return np.identity(dw, dtype=\"int32\")[x_class].T\n",
        "\n",
        "def ReLU(x):\n",
        "    return max(x, 0)\n",
        "\n",
        "def RNN(W_hx, W_hh, x, h, b_h):\n",
        "  w_x = np.dot(W_hx, x).T[0]\n",
        "  w_h = np.dot(W_hh, h)\n",
        "  vReLU = np.vectorize(ReLU)\n",
        "  return vReLU(w_x + w_h + b_h)\n",
        "\n",
        "def calculate_y(W_yh, h, b_y):\n",
        "  return softmax(np.dot(W_yh, h) + b_y,)\n",
        "\n",
        "\n",
        "def calculate_RNN(X, W_hx, W_hh, W_yh, b_h, b_y, dh, dw):\n",
        "  h = np.zeros(dh)\n",
        "  for x in X:\n",
        "    h = RNN(W_hx, W_hh, vector_emb(x, dw), h, b_h)\n",
        "  y = calculate_y(W_yh, h, b_y)\n",
        "  return y\n",
        "\n",
        "dw = 300\n",
        "dh = 50\n",
        "W_hx = np.ones((dh, dw))\n",
        "W_hh = np.ones((dh, dh))\n",
        "W_yh = np.ones((len(CATEGORIES), dh))\n",
        "b_h = np.ones(dh)\n",
        "b_y = np.ones(len(CATEGORIES))\n",
        "\n",
        "words = [1,2,3,4,5]\n",
        "calculate_RNN(words, W_hx, W_hh, W_yh, b_h, b_y, dh, dw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kApUNHf2ITc",
        "outputId": "16358901-a052-461d-de8d-f9044a1ce75d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25, 0.25, 0.25, 0.25])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 82 \n",
        "\n",
        "dw = 300\n",
        "dh = 50\n",
        "W_hx = np.ones((dh, dw))\n",
        "W_hh = np.ones((dh, dh))\n",
        "W_yh = np.ones((len(CATEGORIES), dh))\n",
        "b_h = np.ones(dh)\n",
        "b_y = np.ones(len(CATEGORIES))\n",
        "eta = 10.0\n",
        "\n",
        "with open('./test.txt', mode='r') as f:\n",
        "    data = f.readlines()\n",
        "    for row in data:\n",
        "      try:\n",
        "        X = [get_id_by_word(word) for word in row.split('\\t')[2].strip(\"'\").split(' ')]\n",
        "        y = vector_emb(CATEGORIES.index(row.split('\\t')[5]), len(CATEGORIES)).T[0]\n",
        "        y_hat = calculate_RNN(words, W_hx, W_hh, W_yh, b_h, b_y, dh, dw)\n",
        "        W_hx = W_hx + eta * (y_hat - y) * X\n",
        "      except:\n",
        "        continue\n",
        "\n"
      ],
      "metadata": {
        "id": "FJb2r6ku_fa0"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.ones((2, 5))\n",
        "b = np.ones((5, 1))\n",
        "np.dot(a, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7TpIYFN8haS",
        "outputId": "996b41e1-ffe5-4612-c9a5-3e8028b7c082"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.],\n",
              "       [5.]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    }
  ]
}